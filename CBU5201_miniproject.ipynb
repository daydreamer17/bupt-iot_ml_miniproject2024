{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "351c195f",
   "metadata": {},
   "source": [
    "# Authenticity Analysis of Audio Data Based on CNN Networks Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ed1e27",
   "metadata": {},
   "source": [
    "# 1 Author"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a8434cc",
   "metadata": {},
   "source": [
    "**Student Name**:**Xiongjie Tang** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b975e46",
   "metadata": {},
   "source": [
    "**Student ID**:**221169210**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13712992",
   "metadata": {},
   "source": [
    "# 2 Problem formulation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18357c9",
   "metadata": {},
   "source": [
    "With the rapid development of information technology, audio data plays an important role in various application scenarios such as voice assistants, customer service, and online education. However, the information implicit in audio data is not limited to the language content, but also includes non-language features such as the speaker's tone, speaking speed, and pauses. These non-language features are particularly important in tasks such as deception detection, because deception is often accompanied by subtle changes in speech. Therefore, how to effectively extract valuable features from audio and text bimodal data to accurately identify true and false information in audio has become a challenging research topic. In this project, we aim to develop a CNN deep learning model that combines audio features and language types to accurately distinguish true from false audio content."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1132522d",
   "metadata": {},
   "source": [
    "# 3 Methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71babce5",
   "metadata": {},
   "source": [
    "In the training and validation tasks, we divide the prepared dataset into training set, validation set and test set, with the training set accounting for 60%, the validation set accounting for 15%, and the test set accounting for 25%, to ensure that the model can effectively monitor its generalization ability during the training process and evaluate its performance in the final test stage. The model uses the Adam optimizer, the learning rate is set to 1e-4, and the loss function is selected as binary_crossentropy (binary_crossentropy), which is suitable for binary classification tasks. At the same time, accuracy and AUC are added as evaluation indicators to comprehensively monitor the performance of the model. In order to optimize the training process, Early Stopping is used to monitor the validation loss. If the validation loss does not improve for three consecutive epochs, the training is stopped early and the optimal weight is restored. In addition, the learning rate scheduler (ReduceLROnPlateau) is also used. When the validation loss is no longer reduced, the learning rate is reduced by half to promote further optimization of the model at the local optimal point. In order to deal with the problem of data category imbalance, the category weights are calculated and applied to make the model pay more attention to the minority category during training and improve the recognition rate of the minority category.\n",
    "\n",
    "During the training process, the custom TQDM progress bar callback class is introduced to display the training progress in real time and to calculate and print the F1 score, precision and recall after each epoch to help monitor the model performance on the validation set. In this way, the study is able to keep abreast of the training dynamics of the model and adjust the training strategy as needed.\n",
    "\n",
    "After training, we predict each of the three independently trained models on the test set and average the predictions of each model by simple average integration to obtain the final prediction results. Subsequently, the averaged predictions are converted to binary labels and the overall accuracy, F1 score, precision and recall are calculated to fully evaluate the performance of the integrated models. The aim is to construct an audio classification model with robust performance and accurate classification that can effectively recognise different types of story content."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c6ff25",
   "metadata": {},
   "source": [
    "# 4 Implemented ML prediction pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa76dd36",
   "metadata": {},
   "source": [
    "In this study, we build and implement a complete machine learning prediction pipeline designed to accurately classify story genres by analysing audio data. The prediction pipeline consists of multiple interrelated stages, each of which is responsible for handling a specific task, thus ensuring the efficient transformation of data, the accurate training of models, and the reliability of the final prediction results. The input of the entire pipeline is the original audio file and its corresponding tag information, and the output is the classification prediction result on the test data. The flow of data between stages is transmitted through specific data structures to ensure continuity and consistency of information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c77004-da41-471e-981c-eca53bc20898",
   "metadata": {},
   "source": [
    "First of all we import the necessary libraries and packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "28909f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Embedding, Dense, Flatten, Concatenate\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80e16fb",
   "metadata": {},
   "source": [
    "## 4.1 Transformation stage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a9b4aa",
   "metadata": {},
   "source": [
    "The conversion phase is the first step in the prediction pipeline, responsible for transforming the raw audio data and label information into a format suitable for model input. In this phase, we first load the tag data from the CSV file and preprocess the ‘Story_type’ column by converting it to lowercase and replacing spaces with underscores to ensure tag consistency. Then, we use the tag encoder to convert the different language categories in the ‘Language’ column into numeric codes, so as to facilitate the subsequent model processing.\n",
    "\n",
    "Subsequently, we use the Librosa library to extract features from the audio files by calculating the Mel Spectrogram and converting it to a decibel scale. To ensure that all audio samples are consistent in the time dimension, we fill or crop the Mel Spectrogram to 256 frames in the time dimension. At the same time, a channel dimension is added to accommodate the input requirements of the CNN. After these processes, the audio features, language encoding labels and target labels are stored as NumPy arrays and normalised to remove the differences between different feature measures.\n",
    "\n",
    "After the data preparation, we divide the data into training, validation and test sets with the proportions of 60%, 30% and 10%, respectively. To cope with the category imbalance problem, we calculate the category weights and apply these weights during the training process to ensure that the model treats the categories fairly during training. The outputs of this phase include a normalised array of audio features, an encoded array of language labels, and a corresponding array of target labels, which are used for subsequent model training and validation, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4df3a99",
   "metadata": {},
   "source": [
    "## 4.2 Model stage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df64aa36",
   "metadata": {},
   "source": [
    "In the modelling phase, we designed and trained multiple CNN models to fully exploit the contribution of audio features and linguistic embedding information to story type classification. Each model consists of an audio input layer that accepts a Mayer spectrogram of shape (128, 256, 1), which is converted into a one-dimensional vector via a spreading layer, and a linguistic input layer. The language input layer, on the other hand, accepts numerically encoded language categories, which are converted into low-dimensional embedding vectors through the embedding layer and processed through the spreading layer.\n",
    "\n",
    "The audio feature and linguistic embedding vectors are then spliced to form a composite feature vector. This vector is passed through two fully connected layers with 64 and 32 neurons respectively, both with ReLU activation functions to capture higher order features. Finally, the binary classification results are output through a single neuron layer with a Sigmoid activation function. In order to improve the generalisation ability and robustness of the model, we trained three independent CNN models, each with different batch sizes and training cycles during training to explore the impact of different training configurations on the model performance.\n",
    "\n",
    "During the training process, we employ the Adam optimiser with the learning rate set to 1e-4, use binary cross-entropy as the loss function, and monitor the accuracy and AUC as the evaluation metrics. To ensure the effective learning of the model, we introduce an early stop mechanism to monitor the validation set loss, and if the validation loss no longer decreases for three consecutive cycles, we stop the training and restore the optimal weights. Meanwhile, a learning rate scheduler is used to reduce the learning rate by half when the validation loss no longer decreases within two cycles to help the model jump out of the local optimum. The category weights ensure that the model pays proper attention to minority and majority categories during training by calculating the degree of imbalance in the categories and adjusting for imbalanced data during training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08fe33d4",
   "metadata": {},
   "source": [
    "## 4.3 Ensemble stage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc0c37e",
   "metadata": {},
   "source": [
    "In the integration stage, we aim to improve the overall classification performance by combining the prediction results of multiple models. In this study, we adopt a simple average integration approach to average the prediction results of three independently trained CNN models to obtain more reliable classification results. Specifically, we first predict the three models separately on the test set to obtain three independent predictions. Subsequently, these three predicted values are averaged to obtain the final integrated predicted value.\n",
    "\n",
    "In order to convert the averaged predicted values into binary labels, we set a threshold of 0.5, and samples exceeding this threshold are classified as positive (1), otherwise as negative (0). Finally, we compute the overall accuracy, F1 score, precision, and recall to fully evaluate the performance of the integrated model. Accuracy measures the proportion of samples that are correctly predicted by the model, F1 score integrates precision and recall and is suitable for evaluating classification performance on unbalanced datasets, precision measures the proportion of samples predicted by the model to be positively categorised that are actually positively categorised, and recall measures the proportion of positively categorised samples that can be recognised by the model.\n",
    "\n",
    "With this integrated approach, we are able to leverage the strengths of multiple models and reduce the bias and variance that may exist in a single model, thus improving the overall accuracy and robustness of the predictions. In addition, in the future, we can explore more complex integration methods, such as weighted averaging or stacking, to further optimise the prediction performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4fd29afa-766c-45f5-b82c-dc58a0dc7d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cnn_with_language_embedding(audio_input_shape, num_languages, embedding_dim=8):\n",
    "    # Audio input and processing\n",
    "    audio_input = Input(shape=audio_input_shape, name='audio_input')\n",
    "    audio_features = Flatten()(audio_input)\n",
    "\n",
    "    # Language category input and embedding layer\n",
    "    language_input = Input(shape=(1,), name='language_input')\n",
    "    language_embedding = Embedding(input_dim=num_languages, output_dim=embedding_dim, name='language_embedding')(language_input)\n",
    "    language_features = Flatten()(language_embedding)\n",
    "\n",
    "    # feature fusion\n",
    "    combined_features = Concatenate()([audio_features, language_features])\n",
    "\n",
    "    # Fully Connected Layer and Categorical Output\n",
    "    dense_1 = Dense(64, activation='relu')(combined_features)\n",
    "    dense_2 = Dense(32, activation='relu')(dense_1)\n",
    "    output = Dense(1, activation='sigmoid', name='output')(dense_2)\n",
    "\n",
    "    # build a model\n",
    "    model = Model(inputs=[audio_input, language_input], outputs=output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9667ad98",
   "metadata": {},
   "source": [
    "# 5 Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d90c3f4",
   "metadata": {},
   "source": [
    "First of all we import the necessary libraries and packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "09c63293",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import librosa\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78da42e8",
   "metadata": {},
   "source": [
    "## 5.1 Dataset Description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aeb0770",
   "metadata": {},
   "source": [
    "The datasets for building and evaluating our models will be derived from the MLEnd Deception Dataset. This dataset contains audio recordings of stories along with their corresponding textual transcripts and labels indicating whether the story is true or deceptive. We will create separate training and validation datasets to ensure that our model can be trained effectively and its performance can be accurately evaluated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d17601b",
   "metadata": {},
   "source": [
    "## 5.2 Dataset Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10fc8305",
   "metadata": {},
   "source": [
    "### 5.2.1 Read label data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4063b163",
   "metadata": {},
   "source": [
    "First, we use the pandas.read_csv() function to read the label data, containing the language and label of each story (true_story or deceptive_story), and we notice that the type labels of the stories are not formatted consistently, so we manipulate the text in the Story_type column: converting it to lowercase and replacing spaces with underscores (e.g. True Story is converted to true_story), and then the Story_type column is converted to numeric labels by the apply() method: true_story is mapped to 1 and deceptive_story is mapped to 0 and use LabelEncoder to convert language labels from strings to integers.These labels will be used as target values for the training data。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b467a2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_labels_with_language_encoding(csv_path):\n",
    "    labels_df = pd.read_csv(csv_path)\n",
    "    labels_df['Story_type'] = labels_df['Story_type'].str.lower().str.replace(\" \", \"_\")\n",
    "    labels_df['label'] = labels_df['Story_type'].apply(lambda x: 1 if x == 'true_story' else 0)\n",
    "\n",
    "    # Coding of language categories\n",
    "    label_encoder = LabelEncoder()\n",
    "    labels_df['language_encoded'] = label_encoder.fit_transform(labels_df['Language'])\n",
    "    return labels_df, label_encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ef6346",
   "metadata": {},
   "source": [
    "### 5.2.2 Read audio data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2566d6",
   "metadata": {},
   "source": [
    "Next, we use the librosa library to read the audio file and extract the Mel spectrogram. mel spectrograms are common features of audio signals and are often used in machine learning tasks for speech or audio.\n",
    "n_mels: the number of Mel filters (default 128).\n",
    "hop_length: frame shift parameter that controls the overlap between each frame.\n",
    "n_fft: size of the FFT (Fast Fourier Transform), which affects the resolution of the spectrum.\n",
    "power_to_db(): converts the spectrogram to logarithmic scale\n",
    "We ensure that the time axis part of each Mel spectrogram has a fixed length based on the target length specified by target_length. If the number of columns in the spectrogram is smaller than target_length, it is expanded by padding; if it is larger than the target length, it is cropped. Finally an extra dimension is added, typically used to conform to the input format of a CNN (e.g. a convolutional neural network), here via mel_spec_db[... , np.newaxis] adds a ‘channel’ dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f10cd86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_mel_spectrogram(audio_path, n_mels=128, hop_length=512, n_fft=2048, target_length=256):\n",
    "    try:\n",
    "        y, sr = librosa.load(audio_path, sr=None)\n",
    "        mel_spec = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=n_mels, hop_length=hop_length, n_fft=n_fft)\n",
    "        mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "\n",
    "        # Fill or cut to target length\n",
    "        if mel_spec_db.shape[1] < target_length:\n",
    "            mel_spec_db = np.pad(mel_spec_db, ((0, 0), (0, target_length - mel_spec_db.shape[1])), mode='constant')\n",
    "        else:\n",
    "            mel_spec_db = mel_spec_db[:, :target_length]\n",
    "\n",
    "        # Increased channel dimensions\n",
    "        mel_spec_db = mel_spec_db[..., np.newaxis]\n",
    "        return mel_spec_db\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {audio_path}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "493e5919",
   "metadata": {},
   "source": [
    "### 5.2.3 Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e503b29c",
   "metadata": {},
   "source": [
    "In the data integration and standardization stage, the audio features, language labels, and target labels are combined together through the prepare_data_without_bert function. First, read the data from the label DataFrame line by line, load the audio file according to the file name, extract its Mel spectrogram features, and obtain the encoded language label and classification label. If the audio file reading fails (for example, the path is wrong or the audio is damaged), the sample is skipped. All extracted audio features, language labels, and classification labels are stored in lists respectively. After all samples are processed, these lists are converted to NumPy arrays for subsequent model training. In order to ensure that the model can learn better, the audio features are further standardized. By calculating the mean and standard deviation of each dimension, the feature values ​​of all samples are standardized, and the feature distribution is adjusted to a mean of 0 and a standard deviation of 1. Finally, the standardized audio features, numerical language labels, and target classification labels are returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "be14f5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_without_bert(audio_folder, labels_df, feature_extractor, audio_target_length=256):\n",
    "    audio_features = []\n",
    "    language_labels = []\n",
    "    labels = []\n",
    "\n",
    "    for index, row in labels_df.iterrows():\n",
    "        # Get audio file paths and tags\n",
    "        audio_path = os.path.join(audio_folder, row['filename'])\n",
    "        label = row['label']\n",
    "        language_label = row['language_encoded']  \n",
    "\n",
    "        # Extract audio features\n",
    "        feature = feature_extractor(audio_path, target_length=audio_target_length)\n",
    "        if feature is None:\n",
    "            continue   \n",
    "\n",
    "        # Add features and tags to the list\n",
    "        audio_features.append(feature)\n",
    "        language_labels.append(language_label)\n",
    "        labels.append(label)\n",
    "\n",
    "    # Convert to NumPy array\n",
    "    audio_features = np.array(audio_features)\n",
    "    language_labels = np.array(language_labels, dtype=np.int32)\n",
    "    labels = np.array(labels, dtype=np.int32)\n",
    "\n",
    "    return audio_features, language_labels, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121d00c4",
   "metadata": {},
   "source": [
    "# 6 Experiments and results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db21f90e",
   "metadata": {},
   "source": [
    "First of all we import the necessary libraries and packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c33ca40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.optimizers.legacy import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import class_weight\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import Callback\n",
    "from tqdm import tqdm  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f35c7e",
   "metadata": {},
   "source": [
    "## 6.1 Custom TQDM progress bar callback class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1a6e21",
   "metadata": {},
   "source": [
    "TQDMProgressBar inherits from Keras' Callback and is used to display training progress and calculate F1 score, precision and recall at the end of each epoch.\n",
    "At the end of each epoch, call on_epoch_end to calculate and print the evaluation metrics.\n",
    "At the end of each batch, call on_batch_end to print the loss value for the current batch.\n",
    "At the beginning of training, call on_train_begin to display the progress bar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0ec1611d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TQDMProgressBar(Callback):\n",
    "    def __init__(self, val_data, val_labels):\n",
    "        super(TQDMProgressBar, self).__init__()\n",
    "        self.val_data = val_data\n",
    "        self.val_labels = val_labels\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.epochs = self.params['epochs']\n",
    "        self.epochs_progress = tqdm(total=self.epochs, desc=\"Training Progress\", position=0, ncols=100)\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        self.epochs_progress.update(1)\n",
    "        # Calculate and print F1 Score, Accuracy Rate, Recall Rate\n",
    "        y_pred = self.model.predict(self.val_data)\n",
    "        y_pred_binary = (y_pred > 0.5).astype(int) \n",
    "        f1 = f1_score(self.val_labels, y_pred_binary, zero_division=0)\n",
    "        precision = precision_score(self.val_labels, y_pred_binary, zero_division=0)\n",
    "        recall = recall_score(self.val_labels, y_pred_binary, zero_division=0)\n",
    "\n",
    "        print(f\"Epoch {epoch + 1} - F1 Score: {f1:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}\")\n",
    "\n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        if 'loss' in logs:\n",
    "            tqdm.write(f\"Batch {batch} - Loss: {logs['loss']:.4f}\")\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        self.epochs_progress.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab779e8a",
   "metadata": {},
   "source": [
    "## 6.2 Load data and prepare"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa3433e",
   "metadata": {},
   "source": [
    "First, data needs to be prepared before training begins. In the data preparation stage, the preprocessed audio features, language labels, and target classification labels are divided into training sets, validation sets, and test sets. The division ratio is 60% training set, 20% validation set, and 20% test set. The training set is used to update the model parameters, the validation set is used to evaluate the generalization ability of the model and adjust the hyperparameters, and the test set is used to evaluate the final performance of the model. After dividing the data, the audio features are standardized to ensure that the data distribution is within a stable range, which helps to accelerate model convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81e51d2",
   "metadata": {},
   "source": [
    "Next, three independent CNN models are defined, each with exactly the same structure. Each model consists of two main inputs: audio features and language labels. The audio features are processed by a convolutional neural network (CNN) to extract local features in frequency and time; the language labels are mapped to dense vectors through an embedding layer to represent the semantic information of the language category. These two features are fused in the model, and finally the binary classification task is completed through a fully connected layer, outputting the probability value of each sample belonging to the true_story category. After definition, the three models are compiled using independent Adam optimizers, and the binary_crossentropy loss function and the accuracy and AUC performance indicators are set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f1f2dd",
   "metadata": {},
   "source": [
    "The training part is divided into three independent model trainings. When each model is trained separately, it accepts the audio features and language labels of the training set as input, and the classification label as the target value. The validation set is used to evaluate the performance of the model during the training process to monitor whether overfitting or underfitting occurs. During the training process, the EarlyStopping callback monitors the validation loss. If the validation loss does not improve for three consecutive rounds, the training will stop early. In addition, the ReduceLROnPlateau callback is used to dynamically adjust the learning rate. When the validation loss does not decrease within two rounds, the learning rate will be halved, thereby helping the model to adjust the weight parameters more finely. The training of each model will continue until the early stopping condition is met or the maximum number of training rounds (5 epochs) is reached."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8737c6b",
   "metadata": {},
   "source": [
    "After training, the three models are predicted on the test set. Each model generates a probability value for the test data belonging to the positive class (true_story). These probability values ​​are then integrated to fuse the predictions of the three models by simple averaging. Specifically, for each sample, the average of the predicted probabilities of the three models is taken as the final prediction result. This integration method assumes that each model has expertise in different feature dimensions, and simple averaging can effectively combine the advantages of different models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f32b0ed",
   "metadata": {},
   "source": [
    "Finally, the code converts the probability value of the ensemble prediction into a binary label and applies a threshold of 0.5 to determine the sample category. Subsequently, the code calculates and outputs the performance indicators of the model on the test set, including accuracy, F1 score, precision, and recall. Accuracy indicates the correct rate of the overall classification; F1 score balances precision and recall and is a comprehensive indicator for measuring classification performance; precision focuses on how many of the samples predicted as positive are correct; recall measures how many of the actual positive samples are correctly identified. Through these indicators, the code comprehensively evaluates the classification performance of the ensemble model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bd63dade",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Path settings\n",
    "    audio_folder = './Deception-main/CBU0521DD_stories'\n",
    "    csv_path = './Deception-main/CBU0521DD_stories_attributes.csv'\n",
    "\n",
    "    # Load tags and language encoders\n",
    "    labels_df, label_encoder = load_labels_with_language_encoding(csv_path)\n",
    "\n",
    "    # Data preparation\n",
    "    X_audio, X_language, y = prepare_data_without_bert(\n",
    "        audio_folder, labels_df, extract_mel_spectrogram, audio_target_length=256\n",
    "    )\n",
    "\n",
    "    # Standardised audio features\n",
    "    mean = np.mean(X_audio, axis=0)\n",
    "    std = np.std(X_audio, axis=0)\n",
    "    X_audio = (X_audio - mean) / (std + 1e-9)\n",
    "\n",
    "    # Data segmentation\n",
    "    X_train_audio, X_temp_audio, X_train_language, X_temp_language, y_train, y_temp = train_test_split(\n",
    "        X_audio, X_language, y, test_size=0.4, random_state=42\n",
    "    )\n",
    "    X_val_audio, X_test_audio, X_val_language, X_test_language, y_val, y_test = train_test_split(\n",
    "        X_temp_audio, X_temp_language, y_temp, test_size=0.25, random_state=42\n",
    "    )\n",
    "\n",
    "    # Category weights calculated\n",
    "    class_weights = class_weight.compute_class_weight(\n",
    "        class_weight='balanced',\n",
    "        classes=np.unique(y_train),\n",
    "        y=y_train\n",
    "    )\n",
    "    class_weights_dict = dict(enumerate(class_weights))\n",
    "\n",
    "    # Create models\n",
    "    num_languages = len(label_encoder.classes_)\n",
    "    cnn_model_1 = create_cnn_with_language_embedding(\n",
    "        audio_input_shape=(128, 256, 1),\n",
    "        num_languages=num_languages,\n",
    "        embedding_dim=8  # Embedded dimensions\n",
    "    )\n",
    "    cnn_model_2 = create_cnn_with_language_embedding(\n",
    "        audio_input_shape=(128, 256, 1),\n",
    "        num_languages=num_languages,\n",
    "        embedding_dim=8   \n",
    "    )\n",
    "    cnn_model_3 = create_cnn_with_language_embedding(\n",
    "        audio_input_shape=(128, 256, 1),\n",
    "        num_languages=num_languages,\n",
    "        embedding_dim=8   \n",
    "    )\n",
    "\n",
    "    # Compile each model\n",
    "    optimizer_1= Adam(learning_rate=1e-4)\n",
    "    optimizer_2= Adam(learning_rate=1e-4)\n",
    "    optimizer_3= Adam(learning_rate=1e-4)\n",
    "    cnn_model_1.compile(optimizer=optimizer_1, loss='binary_crossentropy', metrics=['accuracy', 'AUC'])\n",
    "    cnn_model_2.compile(optimizer=optimizer_2, loss='binary_crossentropy', metrics=['accuracy', 'AUC'])\n",
    "    cnn_model_3.compile(optimizer=optimizer_3, loss='binary_crossentropy', metrics=['accuracy', 'AUC'])\n",
    "\n",
    "    # Training callbacks\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "    lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, verbose=1)\n",
    "\n",
    "    # Model training\n",
    "    history_1= cnn_model_1.fit(\n",
    "        [X_train_audio, X_train_language], y_train,\n",
    "        validation_data=([X_val_audio, X_val_language], y_val),\n",
    "        epochs=5,\n",
    "        batch_size=8,\n",
    "        class_weight=class_weights_dict,\n",
    "        callbacks=[early_stopping, lr_scheduler]\n",
    "    )\n",
    "    history_2= cnn_model_2.fit(\n",
    "        [X_train_audio, X_train_language], y_train,\n",
    "        validation_data=([X_val_audio, X_val_language], y_val),\n",
    "        epochs=5,\n",
    "        batch_size=8,\n",
    "        class_weight=class_weights_dict,\n",
    "        callbacks=[early_stopping, lr_scheduler]\n",
    "    )\n",
    "    history_3= cnn_model_3.fit(\n",
    "        [X_train_audio, X_train_language], y_train,\n",
    "        validation_data=([X_val_audio, X_val_language], y_val),\n",
    "        epochs=5,\n",
    "        batch_size=8,\n",
    "        class_weight=class_weights_dict,\n",
    "        callbacks=[early_stopping, lr_scheduler]\n",
    "    )\n",
    "    # Test set predictions\n",
    "    y_pred_1 = cnn_model_1.predict([X_test_audio, X_test_language])\n",
    "    y_pred_2 = cnn_model_2.predict([X_test_audio, X_test_language])\n",
    "    y_pred_3 = cnn_model_3.predict([X_test_audio, X_test_language])\n",
    "    \n",
    "    # Simple average integration\n",
    "    ensemble_predictions = (y_pred_1 + y_pred_2 + y_pred_3) / 3\n",
    "    \n",
    "    # Converted to binary labels\n",
    "    y_pred_binary = (ensemble_predictions > 0.5).astype(int)\n",
    "    \n",
    "    accuracy = np.mean(y_pred_binary.flatten() == y_test.flatten()) * 100\n",
    "    f1 = f1_score(y_test, y_pred_binary)\n",
    "    precision = precision_score(y_test, y_pred_binary)\n",
    "    recall = recall_score(y_test, y_pred_binary)\n",
    "\n",
    "\n",
    "    print(f\"Ensemble Accuracy: {accuracy:.2f}%\")\n",
    "    print(f\"F1 Score: {f1:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbefb4c0",
   "metadata": {},
   "source": [
    "Start training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a3abfd7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "8/8 [==============================] - 3s 50ms/step - loss: 1.3098 - accuracy: 0.3667 - auc: 0.4035 - val_loss: 1.6099 - val_accuracy: 0.4000 - val_auc: 0.3708 - lr: 1.0000e-04\n",
      "Epoch 2/5\n",
      "8/8 [==============================] - 0s 22ms/step - loss: 0.3432 - accuracy: 0.9000 - auc: 0.9492 - val_loss: 1.3521 - val_accuracy: 0.4667 - val_auc: 0.5526 - lr: 1.0000e-04\n",
      "Epoch 3/5\n",
      "8/8 [==============================] - 0s 24ms/step - loss: 0.2531 - accuracy: 0.9000 - auc: 0.9565 - val_loss: 1.5912 - val_accuracy: 0.4333 - val_auc: 0.4282 - lr: 1.0000e-04\n",
      "Epoch 4/5\n",
      "8/8 [==============================] - ETA: 0s - loss: 0.0326 - accuracy: 1.0000 - auc: 1.0000\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "8/8 [==============================] - 0s 24ms/step - loss: 0.0326 - accuracy: 1.0000 - auc: 1.0000 - val_loss: 2.2329 - val_accuracy: 0.4000 - val_auc: 0.3373 - lr: 1.0000e-04\n",
      "Epoch 5/5\n",
      "8/8 [==============================] - 0s 24ms/step - loss: 0.0326 - accuracy: 0.9833 - auc: 1.0000 - val_loss: 2.2682 - val_accuracy: 0.4000 - val_auc: 0.3445 - lr: 5.0000e-05\n",
      "Epoch 1/5\n",
      "8/8 [==============================] - 1s 46ms/step - loss: 1.0481 - accuracy: 0.5167 - auc: 0.3912 - val_loss: 1.2312 - val_accuracy: 0.2667 - val_auc: 0.3541 - lr: 1.0000e-04\n",
      "Epoch 2/5\n",
      "8/8 [==============================] - 0s 23ms/step - loss: 0.3948 - accuracy: 0.8000 - auc: 0.9224 - val_loss: 0.9925 - val_accuracy: 0.5000 - val_auc: 0.6077 - lr: 1.0000e-04\n",
      "Epoch 3/5\n",
      "8/8 [==============================] - 0s 23ms/step - loss: 0.1701 - accuracy: 0.9833 - auc: 1.0000 - val_loss: 1.6757 - val_accuracy: 0.3000 - val_auc: 0.3014 - lr: 1.0000e-04\n",
      "Epoch 4/5\n",
      "5/8 [=================>............] - ETA: 0s - loss: 0.0943 - accuracy: 0.9750 - auc: 0.9975\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 0.1134 - accuracy: 0.9667 - auc: 0.9900 - val_loss: 1.7875 - val_accuracy: 0.3000 - val_auc: 0.3206 - lr: 1.0000e-04\n",
      "Epoch 5/5\n",
      "8/8 [==============================] - 0s 23ms/step - loss: 0.0419 - accuracy: 1.0000 - auc: 1.0000 - val_loss: 1.4474 - val_accuracy: 0.3333 - val_auc: 0.3541 - lr: 5.0000e-05\n",
      "Epoch 1/5\n",
      "8/8 [==============================] - 1s 43ms/step - loss: 1.2671 - accuracy: 0.4333 - auc: 0.3996 - val_loss: 1.2014 - val_accuracy: 0.5333 - val_auc: 0.4880 - lr: 1.0000e-04\n",
      "Epoch 2/5\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 0.2530 - accuracy: 0.9000 - auc: 0.9615 - val_loss: 1.2796 - val_accuracy: 0.5000 - val_auc: 0.4330 - lr: 1.0000e-04\n",
      "Epoch 3/5\n",
      "5/8 [=================>............] - ETA: 0s - loss: 0.1374 - accuracy: 0.9750 - auc: 0.9924\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "8/8 [==============================] - 0s 22ms/step - loss: 0.1025 - accuracy: 0.9833 - auc: 0.9967 - val_loss: 1.5812 - val_accuracy: 0.4667 - val_auc: 0.4354 - lr: 1.0000e-04\n",
      "Epoch 4/5\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 0.0309 - accuracy: 1.0000 - auc: 1.0000 - val_loss: 1.6455 - val_accuracy: 0.4333 - val_auc: 0.4689 - lr: 5.0000e-05\n",
      "WARNING:tensorflow:6 out of the last 14 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001FE65D8B9D0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 66ms/step\n",
      "Ensemble Accuracy: 70.00%\n",
      "F1 Score: 0.8000, Precision: 0.7500, Recall: 0.8571\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c32ce0a-8834-46c2-95c5-65044a8a63c2",
   "metadata": {},
   "source": [
    "## 6.3 Result anaylsis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4add36f0-8403-4c2b-9854-8324dc481f1a",
   "metadata": {},
   "source": [
    "### 6.3.1 Training process analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c665c7-3b72-4cb7-b40d-3a8a9de49057",
   "metadata": {},
   "source": [
    "- **Loss value (Loss)**: During the training phase, the loss value drops rapidly from a high initial value (e.g., 1.3098) to close to 0 (e.g., 0.0326). This indicates that the model has learned features on the training set and is able to fit the training data.\n",
    "- **Accuracy**: The training accuracy quickly rises from an initial low value (such as 36.67%) to 90% or even 100%. This indicates that the model is able to fit the training data well, but because the training accuracy increases so quickly and approaches 100%, this may be a sign of overfitting.\n",
    "- **AUC value**: During the training phase, the AUC rises quickly to close to 1.0 (e.g., 1.0000). An AUC close to 1 indicates that the model is able to distinguish positive and negative samples well, but due to the weak performance of the validation AUC, the improvement in the training AUC may reflect more of the model overfitting the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e21ab63-4c8e-48b1-8373-1db97034c32a",
   "metadata": {},
   "source": [
    "### 6.3.2 Validation process analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1a4b1b-8184-4d28-9f81-8db34df6f499",
   "metadata": {},
   "source": [
    "- **Loss values (val_loss)**: The change in validation loss shows large fluctuations. For example, the validation loss of the first model is 1.3521 in epoch 2, but rises to 2.2682 in epoch 5. This fluctuation suggests that the model may start to overfit in the late stages of training, that is, it tends to memorize the training data rather than learn broadly applicable features.\n",
    "- **Accuracy (val_accuracy)**: The validation accuracy improves slightly from 40.00% in Round 2 in the first model, but stagnates or even decreases in subsequent rounds. For example, in the last round, the validation accuracy is 40.00%, showing that the model does not generalize well to the validation data.\n",
    "- **AUC value (val_auc)**: The validation AUC fluctuates greatly and is generally low. For example, the validation AUC of the first model is 55.26% in the second round, but then continues to drop to 34.45% in the final round. This indicates that the model fails to effectively distinguish between positive and negative samples on the validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ecdad1b-f81a-4945-a979-ba1acf00d3a9",
   "metadata": {},
   "source": [
    "### 6.3.3 Test result analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f69605-5e01-4e98-aec2-1afed7c88e7f",
   "metadata": {},
   "source": [
    "- **Accuracy (Test Accuracy)**: The accuracy of the integrated model on the test set is 70.00%, which is significantly higher than the performance on the validation set. This shows that the ensemble strategy effectively improves the overall classification performance and weakens the overfitting tendency of a single model.\n",
    "- **F1 Score, Precision and Recall**: The F1 score of the ensemble model is 0.8000, which is a high value. The F1 score is the harmonic mean of precision and recall, and is particularly useful for tasks where the balance between false positives and false negatives is important. A high F1 score indicates that the overall classification performance of the model on the test set is relatively balanced."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d932b76a-8c21-4597-82f3-080ee268a98c",
   "metadata": {},
   "source": [
    "# 7 Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872cf296-ea25-45d6-9994-b98e12aab682",
   "metadata": {},
   "source": [
    "In this task, we built a binary classification task based on audio and language features, with the goal of classifying audio stories into true and false (true_story and deceptive_story). By designing multiple convolutional neural network (CNN) models, processing the Mel spectrogram and language category features of the audio respectively, and fusing the predictions of multiple models through the ensemble learning strategy, the overall classification performance was improved."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91cb0970-9aec-4895-b930-492c45755c4c",
   "metadata": {},
   "source": [
    "## 7.1 Highlights of the task model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00e88d8-bb1f-4b99-913b-0ffa8d6881fb",
   "metadata": {},
   "source": [
    "- **Multi-input model design**:The integration of audio and language feature streams enables the model to extract more comprehensive features from multimodal data.\n",
    "- **Application of integration strategy**：The use of simple average integration strategy effectively alleviates the problem of overfitting of a single model and improves the robustness and generalization ability of the model.\n",
    "- **Reasonable training process**：EarlyStopping and ReduceLROnPlateau are used to dynamically control the training process, which improves training efficiency and reduces unnecessary overfitting risks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3642fb11-3366-4066-8936-7d5407397a58",
   "metadata": {},
   "source": [
    "## 7.2 Problems encountered so far"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e59a45f-4a2b-4716-9c24-fa9223f9719e",
   "metadata": {},
   "source": [
    "-The current data size (100 samples) is severely insufficient to train a model with generalisation capabilities. The insufficient data size also leads to overfitting problems, making the model overfits the details of the training data and is difficult to generalize to the validation set and test set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecfdc1ab",
   "metadata": {},
   "source": [
    "-Inconsistent performance between the validation set and the test set:Due to the insufficient sample size, the validation set may not fully represent the data distribution, resulting in large fluctuations in validation performance, while the test set may happen to be distributed closer to the training set, so the test performance is better than the validation performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ae0957",
   "metadata": {},
   "source": [
    "-From the results, the performance of the validation set is much lower than that of the training set and the test set. This may be due to the difference in the distribution of the validation set and the training set and the test set. This distribution difference will cause the model to be unable to evaluate the actual generalization performance well during the validation phase, thus affecting the effectiveness of the early stopping mechanism and the learning rate adjustment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89adbe01",
   "metadata": {},
   "source": [
    "-Although the simple average ensemble strategy improves the overall performance of the model, its potential has not been fully utilized:Currently, the three models have exactly the same structure, and only the weight update during the training process is different, and the model diversity is insufficient.There is no weighted integration of the performance of individual models, and only simple averaging is used, which fails to fully utilize the advantages of each model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd39214-3398-460b-ab49-666a2922e670",
   "metadata": {},
   "source": [
    "## 7.3 Directions for improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6dacb10-956e-4c33-a7ad-caa277c12b00",
   "metadata": {},
   "source": [
    "-**Increase the amount of data**:We can enhance the audio data, such as adding noise, time stretching, pitch changes, etc., to increase data diversity,\n",
    "or get more samples:Expand the data source and get more story audio samples to improve the generalization ability of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5446bdd1-bf8d-4aad-b5d0-080c75563ab3",
   "metadata": {},
   "source": [
    "-**Optimize validation set division**:We can ensure that the proportion of each type of sample in the training, validation and test sets is consistent,\n",
    "or evaluate model performance through K-fold cross-validation to reduce the impact of uneven distribution of validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d7e05b",
   "metadata": {},
   "source": [
    "-**Model architecture**:More complex neural network structures such as Deep CNN, RNN or Self-Attention Mechanism can be explored to capture temporal information and long-range dependencies in audio data. In addition, combining multimodal learning approaches to fuse audio features with other types of data (e.g., text or metadata) may further enhance the classification performance. In order to optimise the model training process, one can try to use more advanced optimisers and learning rate scheduling strategies, or adopt a migration learning approach, using models pre-trained on large-scale audio data as a basis for fine-tuning to suit specific classification tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a5806a",
   "metadata": {},
   "source": [
    "# 8 References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "226362ff",
   "metadata": {},
   "source": [
    "For more details,I have upload the structured code to the github you can check it by enter the link below:\n",
    "\n",
    "https://github.com/daydreamer17/bupt-iot_ml_miniproject2024.git"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolov8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
